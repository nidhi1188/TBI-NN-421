{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TBI-Group9-UNET.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3tpRHd5-41Cu",
        "colab_type": "code",
        "outputId": "14c29831-3f18-4013-f514-93d88cb899bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install pillow\n",
        "import json\n",
        "import pandas as pd\n",
        "import tqdm as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1vupi9GeVq0H",
        "colab_type": "code",
        "outputId": "ef08f185-d997-49c0-ff8d-dbd5529ae318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"twoface262\",\"key\":\"453e89deca1ef616f15f5725eed93000\"}\n",
        "\n",
        "with open('kaggle.json', 'w') as kaggle_json:\n",
        "  json.dump(api_token, kaggle_json)\n",
        "  \n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Dataset source: https://www.kaggle.com/c/human-protein-atlas-image-classification\n",
        "# Attributes: Human Protein Atlas\n",
        "!kaggle competitions download -c human-protein-atlas-image-classification\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/446k [00:00<?, ?B/s]\n",
            "100% 446k/446k [00:00<00:00, 60.7MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/1.22M [00:00<?, ?B/s]\n",
            "100% 1.22M/1.22M [00:00<00:00, 81.2MB/s]\n",
            "Downloading test.zip to /content\n",
            "100% 4.36G/4.37G [00:49<00:00, 98.0MB/s]\n",
            "100% 4.37G/4.37G [00:49<00:00, 94.5MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 13.1G/13.1G [03:00<00:00, 78.3MB/s]\n",
            "100% 13.1G/13.1G [03:00<00:00, 77.8MB/s]\n",
            "kaggle.json  sample_data  sample_submission.csv  test.zip  train.csv  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x26mSbiN8CMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data_train\n",
        "!unzip train.zip -d data_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRayhjX2_fbJ",
        "colab_type": "code",
        "outputId": "6c714495-a7a2-4046-c13c-541ea40155d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_train   sample_data\t    test.zip   train.zip\n",
            "kaggle.json  sample_submission.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LZtcBLmZ_eyk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Authors: Wezley Sherman\n",
        "#\n",
        "# Reference Attributes: Pydicom documentation, glob documentation\n",
        "# https://pydicom.github.io/pydicom/stable/getting_started.html\n",
        "# https://pymotw.com/2/glob/\n",
        "#\n",
        "# This class is a part of the BSSCS Net Framework to import DICOM images\n",
        "#\n",
        "# BSSCS Docs Importer location: BSSCS_DOCS/dicom.html\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class UNET_DATA:\n",
        "\tdef __init__(self,label_classes, batch_size=10, labels_arr=None, image_arr=None, csv_path=None, images_path=None):\n",
        "\t\tself.current_batch = 0\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.total_batches = 10\n",
        "\t\tself.batch_size = 10\n",
        "\t\tself.labels = labels_arr\n",
        "\t\tself.images = image_arr\n",
        "\t\tself.label_classes = label_classes\n",
        "\n",
        "\t\tif csv_path and images_path:\n",
        "\t\t\tprint(\"Found CSV\")\n",
        "\t\t\tdata_frame = self.open_csv(csv_path)\n",
        "\t\t\tself.data_dictionary = self.fetch_images_with_csv(images_path, data_frame)\n",
        "\t\t\tself.data_keys = list(self.data_dictionary.keys())\n",
        "\t\telse:\n",
        "\t\t\tprint(\"No CSV\")\n",
        "\t\t\tself.data_dictionary = None\n",
        "\n",
        "\tdef set_batch_size(self, new_size):\n",
        "\t\t''' Responsible for setting a new batch size\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- new_size: int -- corresponds to the new batch size we want to assign\n",
        "\t\t'''\n",
        "\t\tself.batch_size = new_size\n",
        "\n",
        "\tdef get_batch_size(self):\n",
        "\t\t''' Responsible for returning the batch size for the class\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t-int -- corresponds to the batch size\n",
        "\t\t'''\n",
        "\t\treturn self.batch_size\n",
        "\n",
        "\tdef get_total_batches(self):\n",
        "\t\t''' Responsible for returning how many batches of data are in our dataset\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- int -- corresponds to the number of batches in our dataset\n",
        "\t\t'''\n",
        "\t\treturn math.floor(self.images/batch_size)\n",
        "\n",
        "\t\n",
        "\tdef get_next_batch(self):\n",
        "\t\t'''\tResponsible for batching the data arrays and returning them\n",
        "\t\t\n",
        "\t\t\tReturns: \n",
        "\t\t\t\tlabel_batch: arr -- batch of labels for the associated image\n",
        "\t\t\t\timage_batch: arr -- batch of images for the associated labels\n",
        "\t\t'''\n",
        "\t\tstart_pos = (self.batch_size * self.current_batch)\n",
        "\t\tend_pos =  (self.batch_size * self.current_batch+1)\n",
        "\n",
        "\t\tlabel_batch = []\n",
        "\t\timage_batch = []\n",
        "\t\tif not self.data_dictionary:\n",
        "\t\t\tlabel_batch = self.labels[start_pos:end_pos]\n",
        "\t\t\timage_batch = self.images[start_pos:end_pos]\n",
        "\t\telse:\n",
        "\t\t\tlabel_batch_keys = self.data_keys[start_pos:end_pos]\n",
        "\t\t\tfor key in label_batch_keys:\n",
        "\t\t\t\timage_batch.append(np.resize(np.array(self.data_dictionary[key]['image_arr']), (256, 256, 1)))\n",
        "\t\t\t\tlabel_batch.append(self.data_dictionary[key]['labels'])\n",
        "\n",
        "\t\t# Reset the current batch once we've iterated through all of our data\n",
        "\t\tself.current_batch += 1\n",
        "\t\tif(self.current_batch >= self.total_batches):\n",
        "\t\t\tself.current_batch = 0\n",
        "\n",
        "\t\treturn label_batch, image_batch\n",
        "\t\t\n",
        "\tdef fetch_data(self, path_to_csv):\n",
        "\t\t''' Handles fetching the data from the DICOM Importer\n",
        "\t\t\n",
        "\t\t\tAssigns:\n",
        "\t\t\t\tself.labels\n",
        "\t\t\t\tself.images\n",
        "\t\t'''\n",
        "\t\timages_arr, labels_arr = self.import_labels_from_csv(path_to_csv)\n",
        "\t\tself.images = images_arr\n",
        "\t\tself.labels = labels_arr\n",
        "\t\t\n",
        "\tdef import_labels_from_csv(self, path):\n",
        "\t\t''' Handles opening a CSV of data and reading in the information to match\n",
        "\t\t\tThe image with the label.\n",
        "\t\t\t\n",
        "\t\t\tInput: \n",
        "\t\t\t\t- path: String -- path to the CSV\n",
        "\t\t\t\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- images: list -- list of image file names\n",
        "\t\t\t\t- labels: list -- list of boolean labels\n",
        "\t\t'''\n",
        "\t\tcsv_dataframe = pd.read_csv(path)\n",
        "\t\timages = list(csv_dataframe['file_name'])\n",
        "\t\tlabels = list(csv_dataframe['has_tbi'])\n",
        "\t\treturn images, labels\n",
        "\n",
        "\tdef open_csv(self, path):\n",
        "\t\t''' Handles opening a labels CSV for the test set and returning the datframe\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- path: String -- path to CSV\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- csv_dataframe: pandas dataframe for labels\n",
        "\n",
        "\t\t'''\n",
        "\t\tcsv_dataframe = pd.read_csv(path)\n",
        "\t\treturn csv_dataframe\n",
        "  \n",
        "\tdef scale_image(self, image, width, height):\n",
        "\t\t''' Handles scaling an image so that it can be fed into the UNET.\n",
        "\n",
        "\t\tInput:\n",
        "\t\t\t\t- image: A PIL Image instance\n",
        "\t\t\t\t- width: the new width we want the image to be\n",
        "\t\t\t\t- height: the new height we want the image to be\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\t- A PIL image instance that has been scaled\n",
        "\n",
        "\t\t\t\t* Will apply antialiasing to the \n",
        "\t\t'''\n",
        "\t\treturn image.resize((width, height), Image.ANTIALIAS)\n",
        "\n",
        "\n",
        "\tdef fetch_images_with_csv(self, path, dataframe):\n",
        "\t\t''' Handles fetching images from a filepath and constructs a dictionary with their labels\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- path: String -- path to data folder\n",
        "\t\t\t\t- dataframe: pandas dataframe\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- Dictionary of data structured as:\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\timage_name : {\n",
        "\t\t\t\t\t\timage_arr: [2D pixel array],\n",
        "\t\t\t\t\t\tlabels: [labels array]\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t'''\n",
        "\t\tdata_dictionary = {}\n",
        "\t\tcount = 0\n",
        "\t\tfor row in tqdm(dataframe.iterrows()):\n",
        "\t\t\tdata_dictionary[row[1][0]] = {}\n",
        "\t\t\timage_path = path +'/' + row[1][0] + '_blue.png'\n",
        "\t\t\timage = list(self.scale_image(Image.open(image_path), 256, 256).getdata())\n",
        "\t\t\tdata_dictionary[row[1][0]]['image_arr'] = image\n",
        "\t\t\tlabel_classes_arr = np.zeros(shape=(28))\n",
        "\t\t\tlabels_in_data = np.array(row[1][1].split(' '))\n",
        "\t\t\tprint(self.label_classes)\n",
        "\t\t\tprint(labels_in_data)\n",
        "\t\t\tfor label in labels_in_data:\n",
        "\t\t\t\tlabel_idx = int(label)\n",
        "\t\t\t\tprint(label_idx)\n",
        "\t\t\t\tprint(labels_in_data.size)\n",
        "\t\t\t\tlabel_classes_arr[label_idx] = 1\n",
        "\t\t\tprint(label_classes_arr)\n",
        "\t\t\tdata_dictionary[row[1][0]]['labels'] = label_classes_arr\n",
        "\t\t\tif count == self.batch_size:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tcount += 1\n",
        "\t\treturn data_dictionary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTMEK-5D_VmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Authors: Wezley Sherman\n",
        "#\n",
        "# Reference Attributes: U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "# Authors: Olaf Ronneberger, Philipp Fischer, and Thomas Brox\n",
        "#\n",
        "# This class is a part of the BSSCS Net Framework to import DICOM images and batch them for the UNET\n",
        "#\n",
        "# BSSCS Docs Importer location: TBD\n",
        "#\n",
        "# Citation:\n",
        "# Ronneberger, et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” \n",
        "#     [Astro-Ph/0005112] A Determination of the Hubble Constant from Cepheid Distances and a Model of the Local Peculiar Velocity Field, \n",
        "#     American Physical Society, 18 May 2015, arxiv.org/abs/1505.04597.\n",
        "# Site:\n",
        "# https://arxiv.org/pdf/1505.04597.pdf\n",
        "import tensorflow as tf\n",
        "\n",
        "class BSSCS_UNET:\n",
        "\tdef __init__(self, iterations, batch_size, data_class, labels_shape=[None, 1], learning_rate=0.0001):\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.iterations = iterations\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.data_class = data_class\n",
        "\t\tself.labels_shape = labels_shape\n",
        "\n",
        "\n",
        "\tdef generate_unet_arch(self, input):\n",
        "\t\t''' Handles generating a TF Implementation of a UNET utilizing the architecture discussed in\n",
        "\t\t    \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" by Ronneberger, Fischer, and Brox\n",
        "\n",
        "\t\t    The architecture for the UNET is not ours and all accrediation goes to Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \n",
        "\t\t    We are not claiming any ownership for the architecture. \n",
        "\t\t    Implementing the UNET arch is comparable to implementing selection sort.\n",
        "\n",
        "\t\t    tf.layers.conv2d docs: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d\n",
        "\t\t\ttf.layers.conv2d_transpost for up convolutions: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose\n",
        "\t\t\ttf.concat for the copy and crop methods: https://www.tensorflow.org/api_docs/python/tf/concat\n",
        "\t\t\ttf.slice for cropping the tensors: https://www.tensorflow.org/api_docs/python/tf/slice\n",
        "\t\t\tHopefully I implemented this correctly  ¯\\_(ツ)_/¯\n",
        "\n",
        "\t\t\tQuick guide on cropping a tensor -..\n",
        "\n",
        "\t\t\tSo after some research through the doc's I found out that we can't just crop it as if it were an image, because we are dealing with Tensors (matricies of data).\n",
        "\n",
        "\t\t\tIn order to crop a tensor we must use TensorFlow's slice function (https://www.tensorflow.org/api_docs/python/tf/slice)\n",
        "\t\t\t\n",
        "\t\t\tHere we are cropping the convolutional layer we are upsampling to be the size of the convolutional layer we are concating to. \n",
        "\t\t\tI'm starting at the base coordinates for the tensor object, and am cropping JUST the images (or filters). Thus why we have [-1, size_x, size_y, -1]. \n",
        "\t\t\tThe '-1' values are there to ensure we are keeping the remaining elements of the dimension (AKA our batch size and number of filters) . \n",
        "\t\t\tFrom TF Docs on the -1 values: \" If size[i] is -1, all remaining elements in dimension i are included in the slice. In other words, this is equivalent to setting: size[i] = input.dim_size(i) - begin[i]\"\n",
        "\n",
        "\t\t\tOnce the tensor is properly cropped (Where each filter is the same size as the tensor we are copying into), we can concat the tensors. \n",
        "\t\t\tThis allows us to copy in all of the previous filters into the current tensor. The final shape will be: [Batch, Img_X, Img_Y, [Filters_A + Filters_B]]\n",
        "\t\t'''\n",
        "\t\t# first block in UNET --> Concat with the final block\n",
        "\t\tconvolution_layer_1 = tf.layers.conv2d(inputs=input, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_2 = tf.layers.conv2d(inputs=convolution_layer_1, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tmax_pooling_layer_1 = tf.layers.max_pooling2d(inputs=convolution_layer_2, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "\t\t# second block in UNET --> Concat with second to final block\n",
        "\t\tconvolution_layer_3 = tf.layers.conv2d(inputs=max_pooling_layer_1, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_4 = tf.layers.conv2d(inputs=convolution_layer_3, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tmax_pooling_layer_2 = tf.layers.max_pooling2d(inputs=convolution_layer_4, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "\t\t# third block in UNET --> Concat with third from final block\n",
        "\t\tconvolution_layer_5 = tf.layers.conv2d(inputs=max_pooling_layer_2, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_6 = tf.layers.conv2d(inputs=convolution_layer_5, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tmax_pooling_layer_3 = tf.layers.max_pooling2d(inputs=convolution_layer_6, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "\t\t# fourth block in UNET --> Concat with fourth from final block\n",
        "\t\tconvolution_layer_7 = tf.layers.conv2d(inputs=max_pooling_layer_3, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_8 = tf.layers.conv2d(inputs=convolution_layer_7, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tmax_pooling_layer_4 = tf.layers.max_pooling2d(inputs=convolution_layer_8, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "\t\t# middle UNET block\n",
        "\t\tconvolution_layer_9 = tf.layers.conv2d(inputs=max_pooling_layer_4, filters=1024, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_10 = tf.layers.conv2d(inputs=convolution_layer_9, filters=1024, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_up_1 = tf.layers.conv2d_transpose(inputs=convolution_layer_10, filters=1024, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\t\t\n",
        "\t\t# fourth from last \n",
        "\t\tconvolution_layer_8 = tf.slice(convolution_layer_8, [0, 0, 0, 0], [-1, convolution_up_1.shape[1], convolution_up_1.shape[2], -1])\n",
        "\t\tconcat_layer_1 = tf.concat([convolution_up_1, convolution_layer_8], axis=3) # Note: Experiment with the axis to ensure it is correct. Are we copying the batches or the filters? -- However; different axis's cause an error.\n",
        "\t\t# print(concat_layer_1.shape) # Comes out to be [Batch_Size, Image_X, Image_Y, (Filters_Conv_8 + Filters_Conv_Up_1)]\n",
        "\t\tconvolution_layer_11 = tf.layers.conv2d(inputs=concat_layer_1, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_12 = tf.layers.conv2d(inputs=convolution_layer_11, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_up_2 = tf.layers.conv2d_transpose(inputs=convolution_layer_12, filters=512, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "\t\t\n",
        "\t\t# third from last\n",
        "\t\tconvolution_layer_6 = tf.slice(convolution_layer_6, [0, 0, 0, 0], [-1, convolution_up_2.shape[1], convolution_up_2.shape[2], -1])\n",
        "\t\tconcat_layer_1 = tf.concat([convolution_up_2, convolution_layer_6], axis=3)\n",
        "\t\tconvolution_layer_13 = tf.layers.conv2d(inputs=convolution_up_2, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_14 = tf.layers.conv2d(inputs=convolution_layer_13, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_up_3 = tf.layers.conv2d_transpose(inputs=convolution_layer_14, filters=256, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "\t\t# second from last\n",
        "\t\tconvolution_layer_4 = tf.slice(convolution_layer_4, [0, 0, 0, 0], [-1, convolution_up_3.shape[1], convolution_up_3.shape[2], -1])\n",
        "\t\tconcat_layer_1 = tf.concat([convolution_up_3, convolution_layer_4], axis=3)\n",
        "\t\tconvolution_layer_15 = tf.layers.conv2d(inputs=convolution_up_3, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_16 = tf.layers.conv2d(inputs=convolution_layer_15, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_up_4 = tf.layers.conv2d_transpose(inputs=convolution_layer_16, filters=128, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "\t\t# last block\n",
        "\t\tconvolution_layer_2 = tf.slice(convolution_layer_2, [0, 0, 0, 0], [-1, convolution_up_4.shape[1], convolution_up_4.shape[2], -1])\n",
        "\t\tconcat_layer_1 = tf.concat([convolution_up_4, convolution_layer_2], axis=3)\n",
        "\t\tconvolution_layer_17 = tf.layers.conv2d(inputs=convolution_up_4, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_18 = tf.layers.conv2d(inputs=convolution_layer_17, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_layer_19 = tf.layers.conv2d(inputs=convolution_layer_18, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "\t\tconvolution_up_5 = tf.layers.conv2d_transpose(inputs=convolution_layer_19, filters=2, kernel_size=[1, 1], strides=1, padding=\"SAME\")\n",
        "\t\tprint(convolution_up_5.shape)\n",
        "\t\tflattened = tf.reshape(convolution_up_5, [-1, 504])\n",
        "\t\treturn flattened\n",
        "\n",
        "\tdef create_regressor(self, input): \n",
        "\t\t''' Handles creating the regressor for the UNET classification\n",
        "\n",
        "\t\t\tParameters:\n",
        "\t\t\t\t- input -- input layer (flattened layer from UNET) \n",
        "\t\t\tReturns:\n",
        "\t\t\t \t- Tensor -- last layer in the regressor\n",
        "\t\t'''\n",
        "\t\treg_input = tf.layers.dense(inputs=input, units=504, activation=tf.nn.relu)\n",
        "\t\treg_hidden = tf.layers.dense(inputs=reg_input, units=1750, activation=tf.nn.relu)\n",
        "\t\treg_hidden1 = tf.layers.dense(inputs=reg_hidden, units=2000, activation=tf.nn.relu)\n",
        "\t\treg_hidden2 = tf.layers.dense(inputs=reg_hidden1, units=2500, activation=tf.nn.relu)\n",
        "\t\treg_out = tf.layers.dense(inputs=reg_hidden2, units=28)\n",
        "\t\treturn reg_out\n",
        "\n",
        "\tdef create_loss(self, input, labels):\n",
        "\t\t''' Handles creating a loss function and returning it to the optimizer\n",
        "\n",
        "\t\t\tParameters:\n",
        "\t\t\t \t- Input: The final layer in the graph we are computing the loss for\n",
        "\t\t\t \t- Labels: The labels for the batch we are computing the loss for\n",
        "\n",
        "\t\t\t Returns:\n",
        "\t\t\t \t- Defined loss function\n",
        "\n",
        "\t\t\tTensorFlow documentation: \n",
        "\t\t\thttps://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
        "\t\t'''\n",
        "\t\treturn tf.nn.softmax_cross_entropy_with_logits_v2(logits=input, labels=labels)\n",
        "\n",
        "\tdef create_optimizer(self, input, labels):\n",
        "\t\t'''\n",
        "\n",
        "\t\t\tTensorFlow documentation: \n",
        "\t\t\thttps://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
        "\t\t'''\n",
        "\t\tloss = tf.reduce_mean(self.create_loss(input, labels))\n",
        "\t\treturn tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
        "\n",
        "\tdef train_unet(self):\n",
        "\t\t''' Handles training a UNET based off the data fed to it\n",
        "\t\t'''\n",
        "\n",
        "\n",
        "\t\t# This is where I would put my loss and optimization functions -..\n",
        "\t\t# ..\n",
        "\t\t# ..\n",
        "\t\t# IF I HAD ONE!\n",
        "\t\t#\n",
        "\t\t# Meme Reference: https://www.youtube.com/watch?v=ciWPFvLS5IY\n",
        "\t\t#\n",
        "\t\t# On a serious note -.. Here is where we will plug in the deep regressor once that's built.\n",
        "\t\t# After a UNET run the image will be passed to the deep regressor.\n",
        "\t\t# The regressor will contain the loss function we are optimizing to.\n",
        "\t\tinput_ph = tf.placeholder(tf.float32, shape=[None, 256, 256, 1]) # Placeholder vals were given by paper in initial layer -- these numbers were referenced from the paper.\n",
        "\t\tconv_input = self.generate_unet_arch(input_ph)\n",
        "\t\tclassifier = self.create_regressor(conv_input)\n",
        "\t\tlabels_placeholder = tf.placeholder(tf.float32, shape=self.labels_shape)\n",
        "\t\toptimizer = self.create_optimizer(classifier, labels_placeholder)\n",
        "\t\tloss = self.create_loss(classifier, labels_placeholder)\n",
        "\t\twith tf.Session() as session:\n",
        "\t\t\ttf.global_variables_initializer().run()\n",
        "\t\t\tfor iteration in range(0, self.iterations): # counts for epochs -- or how many times we go through our data\n",
        "\t\t\t\tfor batch in range(0, self.batch_size):\n",
        "\t\t\t\t\ty_b, X_b = self.data_class.get_next_batch()\n",
        "\t\t\t\t\tprint(\"X shape: \", X_b)\n",
        "\t\t\t\t\tprint(\"Y shape: \", y_b)\n",
        "\t\t\t\t\tsession.run(optimizer, feed_dict={input_ph:X_b, labels_placeholder:y_b})\n",
        "\t\t\t\tif iteration % 10 == 0:\n",
        "\t\t\t\t\tprint(\"Iteration: \", iteration)\n",
        "\t\t\t\t\tit_loss = session.run(loss, feed_dict={input_ph:X_b, labels_placeholder:y_b})\n",
        "\t\t\t\t\tlabels = session.run(classifier, feed_dict={input_ph: X_b})\n",
        "\t\t\t\t\tprint(\"Predicted labels: \", labels)\n",
        "\t\t\t\t\tprint(\"Labels actual: \", y_b)\n",
        "\t\t\t\t\t# Evaluate mse loss here and print the value\n",
        "\t\t\t\t\tprint(\"Passed 100 iterations with mse: \", it_loss)\n",
        "\t\t\n",
        "\n",
        "\tdef generalize_prediction(output):\n",
        "\t\t''' Handles generalizing UNET outputs to the labels so that we can better understand the predictions.\n",
        "\t\t\tIf the prediction is under 50% then we mark it as 0, otherwise keep the value.\n",
        "\n",
        "\t\t\tInput: \n",
        "\t\t\t\t- output - array of outputs based off labels from UNET\n",
        "\n",
        "\t\t\tReturn:\n",
        "\t\t\t\t- return_prediction - array of normalized outputs from output input\n",
        "\t\t'''\n",
        "\t\treturn_prediction = []\n",
        "\t\tfor prediction in output:\n",
        "\t\t\tif(prediction > 0.5):\n",
        "\t\t\t\treturn_prediction.append(prediction)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn_prediction.append(0)\n",
        "\n",
        "\t\treturn return_prediction\n",
        "\n",
        "\tdef test_unet(self, graph_out, input_x):\n",
        "\t\t''' Runs a trained UNET through an evaluation/test phase to detect errors\n",
        "\n",
        "\t\t\tParameters:\n",
        "\t\t\t\t- graph_out: conv2d_tranpose tensor - The last layer in the graph\n",
        "\t\t\t\t- input_x: image_arr  - Image array of shape [None, x, y, 1]\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- output: Returns the output of the unet graph\n",
        "\t\t'''\n",
        "\t\twith tf.Session() as session:\n",
        "\t\t\toutput = session.run(graph_out, feed_dict={input:input_x})\n",
        "\n",
        "\t\treturn output\n",
        "\n",
        "\tdef save_graph(self):\n",
        "\t\t''' Saves a UNET graph\n",
        "\t\t'''\n",
        "\t\t# To-Do: Implement when deep regressor is finished\n",
        "\t\treturn None\n",
        "\n",
        "\tdef load_graph(self):\n",
        "\t\t''' Loads a UNET graph\n",
        "\t\t'''\n",
        "\t\t# To-Do: Implement when deep regressor is finished\n",
        "\t\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcDoVC0wUgzQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWvL-RmfBBst",
        "colab_type": "code",
        "outputId": "37b0b044-be74-42b6-dcd6-f8446b14a4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4961
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def test_unet_training(image_path, csv_path, batch_size, iterations):\n",
        "  unet_data = UNET_DATA(label_classes=None, images_path=image_path, csv_path=csv_path)\n",
        "  unet = BSSCS_UNET(iterations, batch_size, unet_data, labels_shape=[None, 28])\n",
        "  print(unet_data.get_next_batch())\n",
        "  unet.train_unet()\n",
        "\n",
        "test_unet_training(image_path=\"data_train\", csv_path=\"train.csv\", batch_size=1, iterations=1000)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9it [00:00, 84.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found CSV\n",
            "None\n",
            "['16' '0']\n",
            "16\n",
            "2\n",
            "0\n",
            "2\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['7' '1' '2' '0']\n",
            "7\n",
            "4\n",
            "1\n",
            "4\n",
            "2\n",
            "4\n",
            "0\n",
            "4\n",
            "[1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['5']\n",
            "5\n",
            "1\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['1']\n",
            "1\n",
            "1\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['18']\n",
            "18\n",
            "1\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['0']\n",
            "0\n",
            "1\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['25' '2']\n",
            "25\n",
            "2\n",
            "2\n",
            "2\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0.]\n",
            "None\n",
            "['0']\n",
            "0\n",
            "1\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['2' '0']\n",
            "2\n",
            "2\n",
            "0\n",
            "2\n",
            "[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['7']\n",
            "7\n",
            "1\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "None\n",
            "['23']\n",
            "23\n",
            "1\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0.]\n",
            "([array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])], [array([[[ 4],\n",
            "        [12],\n",
            "        [11],\n",
            "        ...,\n",
            "        [10],\n",
            "        [10],\n",
            "        [ 0]],\n",
            "\n",
            "       [[12],\n",
            "        [ 8],\n",
            "        [ 8],\n",
            "        ...,\n",
            "        [11],\n",
            "        [ 2],\n",
            "        [ 0]],\n",
            "\n",
            "       [[11],\n",
            "        [ 4],\n",
            "        [ 8],\n",
            "        ...,\n",
            "        [ 6],\n",
            "        [ 1],\n",
            "        [ 0]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0],\n",
            "        [ 0],\n",
            "        [ 0],\n",
            "        ...,\n",
            "        [ 0],\n",
            "        [ 0],\n",
            "        [ 0]],\n",
            "\n",
            "       [[ 3],\n",
            "        [ 0],\n",
            "        [ 1],\n",
            "        ...,\n",
            "        [ 0],\n",
            "        [ 0],\n",
            "        [ 0]],\n",
            "\n",
            "       [[ 4],\n",
            "        [ 2],\n",
            "        [ 3],\n",
            "        ...,\n",
            "        [ 0],\n",
            "        [ 0],\n",
            "        [ 0]]])])\n",
            "WARNING:tensorflow:From <ipython-input-1-aae35a00bd6b>:41: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-aae35a00bd6b>:43: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-aae35a00bd6b>:63: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 252, 252, 2)\n",
            "WARNING:tensorflow:From <ipython-input-1-aae35a00bd6b>:107: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "X shape:  [array([[[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [ 65],\n",
            "        [ 83],\n",
            "        [ 88]],\n",
            "\n",
            "       [[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [ 81],\n",
            "        [ 67],\n",
            "        [110]],\n",
            "\n",
            "       [[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [ 76],\n",
            "        [ 77],\n",
            "        [115]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [  1],\n",
            "        [  0],\n",
            "        [  0]],\n",
            "\n",
            "       [[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [  0],\n",
            "        [  0],\n",
            "        [  0]],\n",
            "\n",
            "       [[  0],\n",
            "        [  0],\n",
            "        [  0],\n",
            "        ...,\n",
            "        [  0],\n",
            "        [  0],\n",
            "        [  0]]])]\n",
            "Y shape:  [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])]\n",
            "Iteration:  0\n",
            "Predicted labels:  [[-0.00466355  0.00279061 -0.00506978 ... -0.00525254 -0.00456001\n",
            "   0.00650518]\n",
            " [-0.00587703  0.00491824 -0.00938744 ... -0.00965682 -0.01273482\n",
            "   0.01476627]\n",
            " [-0.00895697  0.00687919 -0.01557722 ... -0.01323802 -0.01763646\n",
            "   0.02145567]\n",
            " ...\n",
            " [-0.01689026  0.05692141 -0.10155941 ... -0.00331287 -0.01474142\n",
            "   0.07189474]\n",
            " [-0.00460155  0.05076825 -0.10295576 ...  0.00157903 -0.00392754\n",
            "   0.04457027]\n",
            " [-0.0202486   0.02044382 -0.03258248 ...  0.00189124  0.00012119\n",
            "   0.0413568 ]]\n",
            "Labels actual:  [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])]\n",
            "Passed 100 iterations with mse:  [3.3029428 3.266594  3.2403774 3.209542  3.166138  3.1449983 3.1207373\n",
            " 3.117774  3.1138744 3.1146238 3.1170657 3.1204457 3.124443  3.128713\n",
            " 3.1346304 3.1420326 3.150567  3.1593437 3.1694474 3.1798503 3.1923006\n",
            " 3.207355  3.223358  3.2415078 3.254403  3.263107  3.2654216 3.2577293\n",
            " 3.245196  3.2270176 3.2096944 3.1923609 3.1727939 3.1540375 3.1368637\n",
            " 3.1202397 3.1033483 3.0869315 3.072407  3.06084   3.05013   3.0375054\n",
            " 3.022248  3.0077662 2.9931097 2.9771585 2.9647884 2.9520426 2.9420218\n",
            " 2.9357862 2.9312158 2.9292502 2.927041  2.9275637 2.9321961 2.9408042\n",
            " 2.9549649 2.972413  2.9926128 3.009476  3.0208118 3.0281458 3.0350356\n",
            " 3.0418499 3.0431771 3.0456905 3.0433888 3.036437  3.0318713 3.0295799\n",
            " 3.028128  3.0273461 3.0257604 3.0236182 3.0229192 3.0180094 3.0138743\n",
            " 3.0072298 2.9972773 2.9874854 2.9815242 2.976125  2.9748375 2.973996\n",
            " 2.9778051 2.9811873 2.986685  2.995026  3.010995  3.0288482 3.0504842\n",
            " 3.0748997 3.0952156 3.1059039 3.112772  3.113401  3.1132023 3.1095018\n",
            " 3.1044056 3.1012769 3.0985885 3.0957832 3.0949192 3.094987  3.0955443\n",
            " 3.0975053 3.0995545 3.1016483 3.1068764 3.114592  3.1182733 3.1230004\n",
            " 3.127218  3.130937  3.1336117 3.1295137 3.1214647 3.1136599 3.1008139\n",
            " 3.0909827 3.08154   3.0716348 3.065199  3.062336  3.0640242 3.0677261\n",
            " 3.0755095 3.0810623 3.0845962 3.086872  3.0893495 3.09089   3.0917435\n",
            " 3.0937512 3.0975804 3.1012375 3.10345   3.1055384 3.1074088 3.1092534\n",
            " 3.1107314 3.1102083 3.108903  3.1079597 3.1046412 3.102896  3.1002846\n",
            " 3.0999403 3.1011915 3.1045256 3.1030402 3.0973017 3.0914636 3.081972\n",
            " 3.0745912 3.0726478 3.071464  3.0724387 3.0721648 3.0719628 3.0744693\n",
            " 3.0797148 3.0840032 3.0908906 3.0982654 3.1036277 3.109992  3.1139617\n",
            " 3.119275  3.1234894 3.1265624 3.1252248 3.1241    3.125293  3.1277957\n",
            " 3.1297371 3.1320012 3.1344185 3.1372106 3.139792  3.1377373 3.1290524\n",
            " 3.119041  3.1055832 3.0903547 3.0746884 3.059566  3.0459604 3.0359273\n",
            " 3.0291376 3.02166   3.013718  3.0089414 3.00683   3.0023417 3.0003793\n",
            " 2.9985845 3.0007286 3.0014458 2.9999259 3.000436  2.997101  2.9940083\n",
            " 2.9926035 2.9899693 2.985046  2.9849281 2.9861784 2.9858675 2.9853494\n",
            " 2.9840252 2.9810536 2.9800215 2.9790053 2.9771202 2.9757268 2.9725592\n",
            " 2.9709234 2.9718308 2.9746606 2.980373  2.9839594 2.9861689 2.9902933\n",
            " 2.9920676 2.989641  2.9902236 2.988639  2.99002   2.994187  3.0009778\n",
            " 3.0049226 3.0094435 3.019301  3.0294106 3.039522  3.053446  3.0673282\n",
            " 3.0770297 3.0772223 3.0746336 3.0695655 3.0621228 3.0544262 3.0495217\n",
            " 3.0464518 3.0421925 3.0226984 3.029741  3.0456357 3.064298  3.171546 ]\n",
            "X shape:  []\n",
            "Y shape:  []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3f9155b00b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_unet_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c3f9155b00b2>\u001b[0m in \u001b[0;36mtest_unet_training\u001b[0;34m(image_path, csv_path, batch_size, iterations)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBSSCS_UNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_unet_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-aae35a00bd6b>\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (0,) for Tensor 'Placeholder:0', which has shape '(?, 256, 256, 1)'"
          ]
        }
      ]
    }
  ]
}