{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TBI-Group9-UNET.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3tpRHd5-41Cu",
        "colab_type": "code",
        "outputId": "78d50279-c2ba-4ca6-e85f-18237bf30c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install pillow\n",
        "import json\n",
        "import pandas as pd\n",
        "import tqdm as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1vupi9GeVq0H",
        "colab_type": "code",
        "outputId": "3a1a9aaf-9060-4a52-f9c5-47af706e18cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"twoface262\",\"key\":\"453e89deca1ef616f15f5725eed93000\"}\n",
        "\n",
        "with open('kaggle.json', 'w') as kaggle_json:\n",
        "  json.dump(api_token, kaggle_json)\n",
        "  \n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Dataset source: https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels\n",
        "# Attributes: Jaeger S, Karargyris A, Candemir S, Folio L, Siegelman J, Callaghan F, Xue Z, Palaniappan K, Singh RK, Antani S, Thoma G, Wang YX, Lu PX, McDonald CJ. Automatic tuberculosis screening using chest radiographs. IEEE Trans Med Imaging. 2014 Feb;33(2):233-45. doi: 10.1109/TMI.2013.2284099. PMID: 24108713 Candemir S, Jaeger S, Palaniappan K, Musco JP, Singh RK, Xue Z, Karargyris A, Antani S, Thoma G, McDonald CJ. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Trans Med Imaging. 2014 Feb;33(2):577-90. doi: 10.1109/TMI.2013.2290491. PMID: 24239990 Montgomery County X-ray Set X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays are abnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities, including effusions and miliary patterns. The data set includes radiology readings available as a text file.\n",
        "!kaggle datasets download nikhilpandey360/chest-xray-masks-and-labels\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading chest-xray-masks-and-labels.zip to /content\n",
            "100% 4.78G/4.79G [02:03<00:00, 36.2MB/s]\n",
            "100% 4.79G/4.79G [02:03<00:00, 41.8MB/s]\n",
            "chest-xray-masks-and-labels.zip  kaggle.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x26mSbiN8CMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data_train\n",
        "!unzip chest-xray-masks-and-labels.zip -d data_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRayhjX2_fbJ",
        "colab_type": "code",
        "outputId": "d107a150-25cc-41e1-ecc0-322ad64544e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "cell_type": "code",
      "source": [
        "!ls data_train/Lung\\ Segmentation/ClinicalReadings\n",
        "\n",
        "file_x = open(\"data_train/Lung Segmentation/ClinicalReadings/CHNCXR_0613_1.txt\")\n",
        "print(file_x.read())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CHNCXR_0001_0.txt  CHNCXR_0201_0.txt  CHNCXR_0401_1.txt  CHNCXR_0601_1.txt\n",
            "CHNCXR_0002_0.txt  CHNCXR_0202_0.txt  CHNCXR_0402_1.txt  CHNCXR_0602_1.txt\n",
            "CHNCXR_0003_0.txt  CHNCXR_0203_0.txt  CHNCXR_0403_1.txt  CHNCXR_0603_1.txt\n",
            "CHNCXR_0004_0.txt  CHNCXR_0204_0.txt  CHNCXR_0404_1.txt  CHNCXR_0604_1.txt\n",
            "CHNCXR_0005_0.txt  CHNCXR_0205_0.txt  CHNCXR_0405_1.txt  CHNCXR_0605_1.txt\n",
            "CHNCXR_0006_0.txt  CHNCXR_0206_0.txt  CHNCXR_0406_1.txt  CHNCXR_0606_1.txt\n",
            "CHNCXR_0007_0.txt  CHNCXR_0207_0.txt  CHNCXR_0407_1.txt  CHNCXR_0607_1.txt\n",
            "CHNCXR_0008_0.txt  CHNCXR_0208_0.txt  CHNCXR_0408_1.txt  CHNCXR_0608_1.txt\n",
            "CHNCXR_0009_0.txt  CHNCXR_0209_0.txt  CHNCXR_0409_1.txt  CHNCXR_0609_1.txt\n",
            "CHNCXR_0010_0.txt  CHNCXR_0210_0.txt  CHNCXR_0410_1.txt  CHNCXR_0610_1.txt\n",
            "CHNCXR_0011_0.txt  CHNCXR_0211_0.txt  CHNCXR_0411_1.txt  CHNCXR_0611_1.txt\n",
            "CHNCXR_0012_0.txt  CHNCXR_0212_0.txt  CHNCXR_0412_1.txt  CHNCXR_0612_1.txt\n",
            "CHNCXR_0013_0.txt  CHNCXR_0213_0.txt  CHNCXR_0413_1.txt  CHNCXR_0613_1.txt\n",
            "CHNCXR_0014_0.txt  CHNCXR_0214_0.txt  CHNCXR_0414_1.txt  CHNCXR_0614_1.txt\n",
            "CHNCXR_0015_0.txt  CHNCXR_0215_0.txt  CHNCXR_0415_1.txt  CHNCXR_0615_1.txt\n",
            "CHNCXR_0016_0.txt  CHNCXR_0216_0.txt  CHNCXR_0416_1.txt  CHNCXR_0616_1.txt\n",
            "CHNCXR_0017_0.txt  CHNCXR_0217_0.txt  CHNCXR_0417_1.txt  CHNCXR_0617_1.txt\n",
            "CHNCXR_0018_0.txt  CHNCXR_0218_0.txt  CHNCXR_0418_1.txt  CHNCXR_0618_1.txt\n",
            "CHNCXR_0019_0.txt  CHNCXR_0219_0.txt  CHNCXR_0419_1.txt  CHNCXR_0619_1.txt\n",
            "CHNCXR_0020_0.txt  CHNCXR_0220_0.txt  CHNCXR_0420_1.txt  CHNCXR_0620_1.txt\n",
            "CHNCXR_0021_0.txt  CHNCXR_0221_0.txt  CHNCXR_0421_1.txt  CHNCXR_0621_1.txt\n",
            "CHNCXR_0022_0.txt  CHNCXR_0222_0.txt  CHNCXR_0422_1.txt  CHNCXR_0622_1.txt\n",
            "CHNCXR_0023_0.txt  CHNCXR_0223_0.txt  CHNCXR_0423_1.txt  CHNCXR_0623_1.txt\n",
            "CHNCXR_0024_0.txt  CHNCXR_0224_0.txt  CHNCXR_0424_1.txt  CHNCXR_0624_1.txt\n",
            "CHNCXR_0025_0.txt  CHNCXR_0225_0.txt  CHNCXR_0425_1.txt  CHNCXR_0625_1.txt\n",
            "CHNCXR_0026_0.txt  CHNCXR_0226_0.txt  CHNCXR_0426_1.txt  CHNCXR_0626_1.txt\n",
            "CHNCXR_0027_0.txt  CHNCXR_0227_0.txt  CHNCXR_0427_1.txt  CHNCXR_0627_1.txt\n",
            "CHNCXR_0028_0.txt  CHNCXR_0228_0.txt  CHNCXR_0428_1.txt  CHNCXR_0628_1.txt\n",
            "CHNCXR_0029_0.txt  CHNCXR_0229_0.txt  CHNCXR_0429_1.txt  CHNCXR_0629_1.txt\n",
            "CHNCXR_0030_0.txt  CHNCXR_0230_0.txt  CHNCXR_0430_1.txt  CHNCXR_0630_1.txt\n",
            "CHNCXR_0031_0.txt  CHNCXR_0231_0.txt  CHNCXR_0431_1.txt  CHNCXR_0631_1.txt\n",
            "CHNCXR_0032_0.txt  CHNCXR_0232_0.txt  CHNCXR_0432_1.txt  CHNCXR_0632_1.txt\n",
            "CHNCXR_0033_0.txt  CHNCXR_0233_0.txt  CHNCXR_0433_1.txt  CHNCXR_0633_1.txt\n",
            "CHNCXR_0034_0.txt  CHNCXR_0234_0.txt  CHNCXR_0434_1.txt  CHNCXR_0634_1.txt\n",
            "CHNCXR_0035_0.txt  CHNCXR_0235_0.txt  CHNCXR_0435_1.txt  CHNCXR_0635_1.txt\n",
            "CHNCXR_0036_0.txt  CHNCXR_0236_0.txt  CHNCXR_0436_1.txt  CHNCXR_0636_1.txt\n",
            "CHNCXR_0037_0.txt  CHNCXR_0237_0.txt  CHNCXR_0437_1.txt  CHNCXR_0637_1.txt\n",
            "CHNCXR_0038_0.txt  CHNCXR_0238_0.txt  CHNCXR_0438_1.txt  CHNCXR_0638_1.txt\n",
            "CHNCXR_0039_0.txt  CHNCXR_0239_0.txt  CHNCXR_0439_1.txt  CHNCXR_0639_1.txt\n",
            "CHNCXR_0040_0.txt  CHNCXR_0240_0.txt  CHNCXR_0440_1.txt  CHNCXR_0640_1.txt\n",
            "CHNCXR_0041_0.txt  CHNCXR_0241_0.txt  CHNCXR_0441_1.txt  CHNCXR_0641_1.txt\n",
            "CHNCXR_0042_0.txt  CHNCXR_0242_0.txt  CHNCXR_0442_1.txt  CHNCXR_0642_1.txt\n",
            "CHNCXR_0043_0.txt  CHNCXR_0243_0.txt  CHNCXR_0443_1.txt  CHNCXR_0643_1.txt\n",
            "CHNCXR_0044_0.txt  CHNCXR_0244_0.txt  CHNCXR_0444_1.txt  CHNCXR_0644_1.txt\n",
            "CHNCXR_0045_0.txt  CHNCXR_0245_0.txt  CHNCXR_0445_1.txt  CHNCXR_0645_1.txt\n",
            "CHNCXR_0046_0.txt  CHNCXR_0246_0.txt  CHNCXR_0446_1.txt  CHNCXR_0646_1.txt\n",
            "CHNCXR_0047_0.txt  CHNCXR_0247_0.txt  CHNCXR_0447_1.txt  CHNCXR_0647_1.txt\n",
            "CHNCXR_0048_0.txt  CHNCXR_0248_0.txt  CHNCXR_0448_1.txt  CHNCXR_0648_1.txt\n",
            "CHNCXR_0049_0.txt  CHNCXR_0249_0.txt  CHNCXR_0449_1.txt  CHNCXR_0649_1.txt\n",
            "CHNCXR_0050_0.txt  CHNCXR_0250_0.txt  CHNCXR_0450_1.txt  CHNCXR_0650_1.txt\n",
            "CHNCXR_0051_0.txt  CHNCXR_0251_0.txt  CHNCXR_0451_1.txt  CHNCXR_0651_1.txt\n",
            "CHNCXR_0052_0.txt  CHNCXR_0252_0.txt  CHNCXR_0452_1.txt  CHNCXR_0652_1.txt\n",
            "CHNCXR_0053_0.txt  CHNCXR_0253_0.txt  CHNCXR_0453_1.txt  CHNCXR_0653_1.txt\n",
            "CHNCXR_0054_0.txt  CHNCXR_0254_0.txt  CHNCXR_0454_1.txt  CHNCXR_0654_1.txt\n",
            "CHNCXR_0055_0.txt  CHNCXR_0255_0.txt  CHNCXR_0455_1.txt  CHNCXR_0655_1.txt\n",
            "CHNCXR_0056_0.txt  CHNCXR_0256_0.txt  CHNCXR_0456_1.txt  CHNCXR_0656_1.txt\n",
            "CHNCXR_0057_0.txt  CHNCXR_0257_0.txt  CHNCXR_0457_1.txt  CHNCXR_0657_1.txt\n",
            "CHNCXR_0058_0.txt  CHNCXR_0258_0.txt  CHNCXR_0458_1.txt  CHNCXR_0658_1.txt\n",
            "CHNCXR_0059_0.txt  CHNCXR_0259_0.txt  CHNCXR_0459_1.txt  CHNCXR_0659_1.txt\n",
            "CHNCXR_0060_0.txt  CHNCXR_0260_0.txt  CHNCXR_0460_1.txt  CHNCXR_0660_1.txt\n",
            "CHNCXR_0061_0.txt  CHNCXR_0261_0.txt  CHNCXR_0461_1.txt  CHNCXR_0661_1.txt\n",
            "CHNCXR_0062_0.txt  CHNCXR_0262_0.txt  CHNCXR_0462_1.txt  CHNCXR_0662_1.txt\n",
            "CHNCXR_0063_0.txt  CHNCXR_0263_0.txt  CHNCXR_0463_1.txt  MCUCXR_0001_0.txt\n",
            "CHNCXR_0064_0.txt  CHNCXR_0264_0.txt  CHNCXR_0464_1.txt  MCUCXR_0002_0.txt\n",
            "CHNCXR_0065_0.txt  CHNCXR_0265_0.txt  CHNCXR_0465_1.txt  MCUCXR_0003_0.txt\n",
            "CHNCXR_0066_0.txt  CHNCXR_0266_0.txt  CHNCXR_0466_1.txt  MCUCXR_0004_0.txt\n",
            "CHNCXR_0067_0.txt  CHNCXR_0267_0.txt  CHNCXR_0467_1.txt  MCUCXR_0005_0.txt\n",
            "CHNCXR_0068_0.txt  CHNCXR_0268_0.txt  CHNCXR_0468_1.txt  MCUCXR_0006_0.txt\n",
            "CHNCXR_0069_0.txt  CHNCXR_0269_0.txt  CHNCXR_0469_1.txt  MCUCXR_0008_0.txt\n",
            "CHNCXR_0070_0.txt  CHNCXR_0270_0.txt  CHNCXR_0470_1.txt  MCUCXR_0011_0.txt\n",
            "CHNCXR_0071_0.txt  CHNCXR_0271_0.txt  CHNCXR_0471_1.txt  MCUCXR_0013_0.txt\n",
            "CHNCXR_0072_0.txt  CHNCXR_0272_0.txt  CHNCXR_0472_1.txt  MCUCXR_0015_0.txt\n",
            "CHNCXR_0073_0.txt  CHNCXR_0273_0.txt  CHNCXR_0473_1.txt  MCUCXR_0016_0.txt\n",
            "CHNCXR_0074_0.txt  CHNCXR_0274_0.txt  CHNCXR_0474_1.txt  MCUCXR_0017_0.txt\n",
            "CHNCXR_0075_0.txt  CHNCXR_0275_0.txt  CHNCXR_0475_1.txt  MCUCXR_0019_0.txt\n",
            "CHNCXR_0076_0.txt  CHNCXR_0276_0.txt  CHNCXR_0476_1.txt  MCUCXR_0020_0.txt\n",
            "CHNCXR_0077_0.txt  CHNCXR_0277_0.txt  CHNCXR_0477_1.txt  MCUCXR_0021_0.txt\n",
            "CHNCXR_0078_0.txt  CHNCXR_0278_0.txt  CHNCXR_0478_1.txt  MCUCXR_0022_0.txt\n",
            "CHNCXR_0079_0.txt  CHNCXR_0279_0.txt  CHNCXR_0479_1.txt  MCUCXR_0023_0.txt\n",
            "CHNCXR_0080_0.txt  CHNCXR_0280_0.txt  CHNCXR_0480_1.txt  MCUCXR_0024_0.txt\n",
            "CHNCXR_0081_0.txt  CHNCXR_0281_0.txt  CHNCXR_0481_1.txt  MCUCXR_0026_0.txt\n",
            "CHNCXR_0082_0.txt  CHNCXR_0282_0.txt  CHNCXR_0482_1.txt  MCUCXR_0027_0.txt\n",
            "CHNCXR_0083_0.txt  CHNCXR_0283_0.txt  CHNCXR_0483_1.txt  MCUCXR_0028_0.txt\n",
            "CHNCXR_0084_0.txt  CHNCXR_0284_0.txt  CHNCXR_0484_1.txt  MCUCXR_0029_0.txt\n",
            "CHNCXR_0085_0.txt  CHNCXR_0285_0.txt  CHNCXR_0485_1.txt  MCUCXR_0030_0.txt\n",
            "CHNCXR_0086_0.txt  CHNCXR_0286_0.txt  CHNCXR_0486_1.txt  MCUCXR_0031_0.txt\n",
            "CHNCXR_0087_0.txt  CHNCXR_0287_0.txt  CHNCXR_0487_1.txt  MCUCXR_0035_0.txt\n",
            "CHNCXR_0088_0.txt  CHNCXR_0288_0.txt  CHNCXR_0488_1.txt  MCUCXR_0038_0.txt\n",
            "CHNCXR_0089_0.txt  CHNCXR_0289_0.txt  CHNCXR_0489_1.txt  MCUCXR_0040_0.txt\n",
            "CHNCXR_0090_0.txt  CHNCXR_0290_0.txt  CHNCXR_0490_1.txt  MCUCXR_0041_0.txt\n",
            "CHNCXR_0091_0.txt  CHNCXR_0291_0.txt  CHNCXR_0491_1.txt  MCUCXR_0042_0.txt\n",
            "CHNCXR_0092_0.txt  CHNCXR_0292_0.txt  CHNCXR_0492_1.txt  MCUCXR_0043_0.txt\n",
            "CHNCXR_0093_0.txt  CHNCXR_0293_0.txt  CHNCXR_0493_1.txt  MCUCXR_0044_0.txt\n",
            "CHNCXR_0094_0.txt  CHNCXR_0294_0.txt  CHNCXR_0494_1.txt  MCUCXR_0045_0.txt\n",
            "CHNCXR_0095_0.txt  CHNCXR_0295_0.txt  CHNCXR_0495_1.txt  MCUCXR_0046_0.txt\n",
            "CHNCXR_0096_0.txt  CHNCXR_0296_0.txt  CHNCXR_0496_1.txt  MCUCXR_0047_0.txt\n",
            "CHNCXR_0097_0.txt  CHNCXR_0297_0.txt  CHNCXR_0497_1.txt  MCUCXR_0048_0.txt\n",
            "CHNCXR_0098_0.txt  CHNCXR_0298_0.txt  CHNCXR_0498_1.txt  MCUCXR_0049_0.txt\n",
            "CHNCXR_0099_0.txt  CHNCXR_0299_0.txt  CHNCXR_0499_1.txt  MCUCXR_0051_0.txt\n",
            "CHNCXR_0100_0.txt  CHNCXR_0300_0.txt  CHNCXR_0500_1.txt  MCUCXR_0052_0.txt\n",
            "CHNCXR_0101_0.txt  CHNCXR_0301_0.txt  CHNCXR_0501_1.txt  MCUCXR_0053_0.txt\n",
            "CHNCXR_0102_0.txt  CHNCXR_0302_0.txt  CHNCXR_0502_1.txt  MCUCXR_0054_0.txt\n",
            "CHNCXR_0103_0.txt  CHNCXR_0303_0.txt  CHNCXR_0503_1.txt  MCUCXR_0055_0.txt\n",
            "CHNCXR_0104_0.txt  CHNCXR_0304_0.txt  CHNCXR_0504_1.txt  MCUCXR_0056_0.txt\n",
            "CHNCXR_0105_0.txt  CHNCXR_0305_0.txt  CHNCXR_0505_1.txt  MCUCXR_0057_0.txt\n",
            "CHNCXR_0106_0.txt  CHNCXR_0306_0.txt  CHNCXR_0506_1.txt  MCUCXR_0058_0.txt\n",
            "CHNCXR_0107_0.txt  CHNCXR_0307_0.txt  CHNCXR_0507_1.txt  MCUCXR_0059_0.txt\n",
            "CHNCXR_0108_0.txt  CHNCXR_0308_0.txt  CHNCXR_0508_1.txt  MCUCXR_0060_0.txt\n",
            "CHNCXR_0109_0.txt  CHNCXR_0309_0.txt  CHNCXR_0509_1.txt  MCUCXR_0061_0.txt\n",
            "CHNCXR_0110_0.txt  CHNCXR_0310_0.txt  CHNCXR_0510_1.txt  MCUCXR_0062_0.txt\n",
            "CHNCXR_0111_0.txt  CHNCXR_0311_0.txt  CHNCXR_0511_1.txt  MCUCXR_0063_0.txt\n",
            "CHNCXR_0112_0.txt  CHNCXR_0312_0.txt  CHNCXR_0512_1.txt  MCUCXR_0064_0.txt\n",
            "CHNCXR_0113_0.txt  CHNCXR_0313_0.txt  CHNCXR_0513_1.txt  MCUCXR_0068_0.txt\n",
            "CHNCXR_0114_0.txt  CHNCXR_0314_0.txt  CHNCXR_0514_1.txt  MCUCXR_0069_0.txt\n",
            "CHNCXR_0115_0.txt  CHNCXR_0315_0.txt  CHNCXR_0515_1.txt  MCUCXR_0070_0.txt\n",
            "CHNCXR_0116_0.txt  CHNCXR_0316_0.txt  CHNCXR_0516_1.txt  MCUCXR_0071_0.txt\n",
            "CHNCXR_0117_0.txt  CHNCXR_0317_0.txt  CHNCXR_0517_1.txt  MCUCXR_0072_0.txt\n",
            "CHNCXR_0118_0.txt  CHNCXR_0318_0.txt  CHNCXR_0518_1.txt  MCUCXR_0074_0.txt\n",
            "CHNCXR_0119_0.txt  CHNCXR_0319_0.txt  CHNCXR_0519_1.txt  MCUCXR_0075_0.txt\n",
            "CHNCXR_0120_0.txt  CHNCXR_0320_0.txt  CHNCXR_0520_1.txt  MCUCXR_0077_0.txt\n",
            "CHNCXR_0121_0.txt  CHNCXR_0321_0.txt  CHNCXR_0521_1.txt  MCUCXR_0079_0.txt\n",
            "CHNCXR_0122_0.txt  CHNCXR_0322_0.txt  CHNCXR_0522_1.txt  MCUCXR_0080_0.txt\n",
            "CHNCXR_0123_0.txt  CHNCXR_0323_0.txt  CHNCXR_0523_1.txt  MCUCXR_0081_0.txt\n",
            "CHNCXR_0124_0.txt  CHNCXR_0324_0.txt  CHNCXR_0524_1.txt  MCUCXR_0082_0.txt\n",
            "CHNCXR_0125_0.txt  CHNCXR_0325_0.txt  CHNCXR_0525_1.txt  MCUCXR_0083_0.txt\n",
            "CHNCXR_0126_0.txt  CHNCXR_0326_0.txt  CHNCXR_0526_1.txt  MCUCXR_0084_0.txt\n",
            "CHNCXR_0127_0.txt  CHNCXR_0327_1.txt  CHNCXR_0527_1.txt  MCUCXR_0085_0.txt\n",
            "CHNCXR_0128_0.txt  CHNCXR_0328_1.txt  CHNCXR_0528_1.txt  MCUCXR_0086_0.txt\n",
            "CHNCXR_0129_0.txt  CHNCXR_0329_1.txt  CHNCXR_0529_1.txt  MCUCXR_0087_0.txt\n",
            "CHNCXR_0130_0.txt  CHNCXR_0330_1.txt  CHNCXR_0530_1.txt  MCUCXR_0089_0.txt\n",
            "CHNCXR_0131_0.txt  CHNCXR_0331_1.txt  CHNCXR_0531_1.txt  MCUCXR_0090_0.txt\n",
            "CHNCXR_0132_0.txt  CHNCXR_0332_1.txt  CHNCXR_0532_1.txt  MCUCXR_0091_0.txt\n",
            "CHNCXR_0133_0.txt  CHNCXR_0333_1.txt  CHNCXR_0533_1.txt  MCUCXR_0092_0.txt\n",
            "CHNCXR_0134_0.txt  CHNCXR_0334_1.txt  CHNCXR_0534_1.txt  MCUCXR_0094_0.txt\n",
            "CHNCXR_0135_0.txt  CHNCXR_0335_1.txt  CHNCXR_0535_1.txt  MCUCXR_0095_0.txt\n",
            "CHNCXR_0136_0.txt  CHNCXR_0336_1.txt  CHNCXR_0536_1.txt  MCUCXR_0096_0.txt\n",
            "CHNCXR_0137_0.txt  CHNCXR_0337_1.txt  CHNCXR_0537_1.txt  MCUCXR_0097_0.txt\n",
            "CHNCXR_0138_0.txt  CHNCXR_0338_1.txt  CHNCXR_0538_1.txt  MCUCXR_0099_0.txt\n",
            "CHNCXR_0139_0.txt  CHNCXR_0339_1.txt  CHNCXR_0539_1.txt  MCUCXR_0100_0.txt\n",
            "CHNCXR_0140_0.txt  CHNCXR_0340_1.txt  CHNCXR_0540_1.txt  MCUCXR_0101_0.txt\n",
            "CHNCXR_0141_0.txt  CHNCXR_0341_1.txt  CHNCXR_0541_1.txt  MCUCXR_0102_0.txt\n",
            "CHNCXR_0142_0.txt  CHNCXR_0342_1.txt  CHNCXR_0542_1.txt  MCUCXR_0103_0.txt\n",
            "CHNCXR_0143_0.txt  CHNCXR_0343_1.txt  CHNCXR_0543_1.txt  MCUCXR_0104_1.txt\n",
            "CHNCXR_0144_0.txt  CHNCXR_0344_1.txt  CHNCXR_0544_1.txt  MCUCXR_0108_1.txt\n",
            "CHNCXR_0145_0.txt  CHNCXR_0345_1.txt  CHNCXR_0545_1.txt  MCUCXR_0113_1.txt\n",
            "CHNCXR_0146_0.txt  CHNCXR_0346_1.txt  CHNCXR_0546_1.txt  MCUCXR_0117_1.txt\n",
            "CHNCXR_0147_0.txt  CHNCXR_0347_1.txt  CHNCXR_0547_1.txt  MCUCXR_0126_1.txt\n",
            "CHNCXR_0148_0.txt  CHNCXR_0348_1.txt  CHNCXR_0548_1.txt  MCUCXR_0140_1.txt\n",
            "CHNCXR_0149_0.txt  CHNCXR_0349_1.txt  CHNCXR_0549_1.txt  MCUCXR_0141_1.txt\n",
            "CHNCXR_0150_0.txt  CHNCXR_0350_1.txt  CHNCXR_0550_1.txt  MCUCXR_0142_1.txt\n",
            "CHNCXR_0151_0.txt  CHNCXR_0351_1.txt  CHNCXR_0551_1.txt  MCUCXR_0144_1.txt\n",
            "CHNCXR_0152_0.txt  CHNCXR_0352_1.txt  CHNCXR_0552_1.txt  MCUCXR_0150_1.txt\n",
            "CHNCXR_0153_0.txt  CHNCXR_0353_1.txt  CHNCXR_0553_1.txt  MCUCXR_0162_1.txt\n",
            "CHNCXR_0154_0.txt  CHNCXR_0354_1.txt  CHNCXR_0554_1.txt  MCUCXR_0166_1.txt\n",
            "CHNCXR_0155_0.txt  CHNCXR_0355_1.txt  CHNCXR_0555_1.txt  MCUCXR_0170_1.txt\n",
            "CHNCXR_0156_0.txt  CHNCXR_0356_1.txt  CHNCXR_0556_1.txt  MCUCXR_0173_1.txt\n",
            "CHNCXR_0157_0.txt  CHNCXR_0357_1.txt  CHNCXR_0557_1.txt  MCUCXR_0182_1.txt\n",
            "CHNCXR_0158_0.txt  CHNCXR_0358_1.txt  CHNCXR_0558_1.txt  MCUCXR_0188_1.txt\n",
            "CHNCXR_0159_0.txt  CHNCXR_0359_1.txt  CHNCXR_0559_1.txt  MCUCXR_0194_1.txt\n",
            "CHNCXR_0160_0.txt  CHNCXR_0360_1.txt  CHNCXR_0560_1.txt  MCUCXR_0195_1.txt\n",
            "CHNCXR_0161_0.txt  CHNCXR_0361_1.txt  CHNCXR_0561_1.txt  MCUCXR_0196_1.txt\n",
            "CHNCXR_0162_0.txt  CHNCXR_0362_1.txt  CHNCXR_0562_1.txt  MCUCXR_0203_1.txt\n",
            "CHNCXR_0163_0.txt  CHNCXR_0363_1.txt  CHNCXR_0563_1.txt  MCUCXR_0213_1.txt\n",
            "CHNCXR_0164_0.txt  CHNCXR_0364_1.txt  CHNCXR_0564_1.txt  MCUCXR_0218_1.txt\n",
            "CHNCXR_0165_0.txt  CHNCXR_0365_1.txt  CHNCXR_0565_1.txt  MCUCXR_0223_1.txt\n",
            "CHNCXR_0166_0.txt  CHNCXR_0366_1.txt  CHNCXR_0566_1.txt  MCUCXR_0228_1.txt\n",
            "CHNCXR_0167_0.txt  CHNCXR_0367_1.txt  CHNCXR_0567_1.txt  MCUCXR_0243_1.txt\n",
            "CHNCXR_0168_0.txt  CHNCXR_0368_1.txt  CHNCXR_0568_1.txt  MCUCXR_0251_1.txt\n",
            "CHNCXR_0169_0.txt  CHNCXR_0369_1.txt  CHNCXR_0569_1.txt  MCUCXR_0253_1.txt\n",
            "CHNCXR_0170_0.txt  CHNCXR_0370_1.txt  CHNCXR_0570_1.txt  MCUCXR_0254_1.txt\n",
            "CHNCXR_0171_0.txt  CHNCXR_0371_1.txt  CHNCXR_0571_1.txt  MCUCXR_0255_1.txt\n",
            "CHNCXR_0172_0.txt  CHNCXR_0372_1.txt  CHNCXR_0572_1.txt  MCUCXR_0258_1.txt\n",
            "CHNCXR_0173_0.txt  CHNCXR_0373_1.txt  CHNCXR_0573_1.txt  MCUCXR_0264_1.txt\n",
            "CHNCXR_0174_0.txt  CHNCXR_0374_1.txt  CHNCXR_0574_1.txt  MCUCXR_0266_1.txt\n",
            "CHNCXR_0175_0.txt  CHNCXR_0375_1.txt  CHNCXR_0575_1.txt  MCUCXR_0275_1.txt\n",
            "CHNCXR_0176_0.txt  CHNCXR_0376_1.txt  CHNCXR_0576_1.txt  MCUCXR_0282_1.txt\n",
            "CHNCXR_0177_0.txt  CHNCXR_0377_1.txt  CHNCXR_0577_1.txt  MCUCXR_0289_1.txt\n",
            "CHNCXR_0178_0.txt  CHNCXR_0378_1.txt  CHNCXR_0578_1.txt  MCUCXR_0294_1.txt\n",
            "CHNCXR_0179_0.txt  CHNCXR_0379_1.txt  CHNCXR_0579_1.txt  MCUCXR_0301_1.txt\n",
            "CHNCXR_0180_0.txt  CHNCXR_0380_1.txt  CHNCXR_0580_1.txt  MCUCXR_0309_1.txt\n",
            "CHNCXR_0181_0.txt  CHNCXR_0381_1.txt  CHNCXR_0581_1.txt  MCUCXR_0311_1.txt\n",
            "CHNCXR_0182_0.txt  CHNCXR_0382_1.txt  CHNCXR_0582_1.txt  MCUCXR_0313_1.txt\n",
            "CHNCXR_0183_0.txt  CHNCXR_0383_1.txt  CHNCXR_0583_1.txt  MCUCXR_0316_1.txt\n",
            "CHNCXR_0184_0.txt  CHNCXR_0384_1.txt  CHNCXR_0584_1.txt  MCUCXR_0331_1.txt\n",
            "CHNCXR_0185_0.txt  CHNCXR_0385_1.txt  CHNCXR_0585_1.txt  MCUCXR_0334_1.txt\n",
            "CHNCXR_0186_0.txt  CHNCXR_0386_1.txt  CHNCXR_0586_1.txt  MCUCXR_0338_1.txt\n",
            "CHNCXR_0187_0.txt  CHNCXR_0387_1.txt  CHNCXR_0587_1.txt  MCUCXR_0348_1.txt\n",
            "CHNCXR_0188_0.txt  CHNCXR_0388_1.txt  CHNCXR_0588_1.txt  MCUCXR_0350_1.txt\n",
            "CHNCXR_0189_0.txt  CHNCXR_0389_1.txt  CHNCXR_0589_1.txt  MCUCXR_0352_1.txt\n",
            "CHNCXR_0190_0.txt  CHNCXR_0390_1.txt  CHNCXR_0590_1.txt  MCUCXR_0354_1.txt\n",
            "CHNCXR_0191_0.txt  CHNCXR_0391_1.txt  CHNCXR_0591_1.txt  MCUCXR_0362_1.txt\n",
            "CHNCXR_0192_0.txt  CHNCXR_0392_1.txt  CHNCXR_0592_1.txt  MCUCXR_0367_1.txt\n",
            "CHNCXR_0193_0.txt  CHNCXR_0393_1.txt  CHNCXR_0593_1.txt  MCUCXR_0369_1.txt\n",
            "CHNCXR_0194_0.txt  CHNCXR_0394_1.txt  CHNCXR_0594_1.txt  MCUCXR_0372_1.txt\n",
            "CHNCXR_0195_0.txt  CHNCXR_0395_1.txt  CHNCXR_0595_1.txt  MCUCXR_0375_1.txt\n",
            "CHNCXR_0196_0.txt  CHNCXR_0396_1.txt  CHNCXR_0596_1.txt  MCUCXR_0383_1.txt\n",
            "CHNCXR_0197_0.txt  CHNCXR_0397_1.txt  CHNCXR_0597_1.txt  MCUCXR_0387_1.txt\n",
            "CHNCXR_0198_0.txt  CHNCXR_0398_1.txt  CHNCXR_0598_1.txt  MCUCXR_0390_1.txt\n",
            "CHNCXR_0199_0.txt  CHNCXR_0399_1.txt  CHNCXR_0599_1.txt  MCUCXR_0393_1.txt\n",
            "CHNCXR_0200_0.txt  CHNCXR_0400_1.txt  CHNCXR_0600_1.txt  MCUCXR_0399_1.txt\n",
            "female 30yrs  \t\n",
            "Old PTB in the right upper field \t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vTjDf0AEDsvR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to base the label off of the name. 0 indicates normal and 1 indicates PTB.\n",
        "\n",
        "We should first load in the iamges and label them. We can label them as 0 for normal and 1 for PTB (single labeled, but will be fine)"
      ]
    },
    {
      "metadata": {
        "id": "LZtcBLmZ_eyk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Authors: Wezley Sherman\n",
        "#\n",
        "# Reference Attributes: Pydicom documentation, glob documentation\n",
        "# https://pydicom.github.io/pydicom/stable/getting_started.html\n",
        "# https://pymotw.com/2/glob/\n",
        "#\n",
        "# This class is a part of the BSSCS Net Framework to import DICOM images\n",
        "#\n",
        "# BSSCS Docs Importer location: BSSCS_DOCS/dicom.html\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class UNET_DATA:\n",
        "\tdef __init__(self,label_classes, batch_size=2, labels_arr=None, image_arr=None, csv_path=None, images_path=None):\n",
        "\t\tself.current_batch = 0\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.total_batches = 2\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.labels = labels_arr\n",
        "\t\tself.images = image_arr\n",
        "\t\tself.label_classes = label_classes\n",
        "\n",
        "\t\tif csv_path and images_path:\n",
        "\t\t\tprint(\"Found CSV\")\n",
        "\t\t\tdata_frame = self.open_csv(csv_path)\n",
        "\t\t\tself.data_dictionary = self.fetch_images_with_csv(images_path, data_frame)\n",
        "\t\t\tself.data_keys = list(self.data_dictionary.keys())\n",
        "\t\telse:\n",
        "\t\t\tprint(\"No CSV\")\n",
        "\t\t\tself.data_dictionary = None\n",
        "\n",
        "\tdef set_batch_size(self, new_size):\n",
        "\t\t''' Responsible for setting a new batch size\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- new_size: int -- corresponds to the new batch size we want to assign\n",
        "\t\t'''\n",
        "\t\tself.batch_size = new_size\n",
        "\n",
        "\tdef get_batch_size(self):\n",
        "\t\t''' Responsible for returning the batch size for the class\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t-int -- corresponds to the batch size\n",
        "\t\t'''\n",
        "\t\treturn self.batch_size\n",
        "\n",
        "\tdef get_total_batches(self):\n",
        "\t\t''' Responsible for returning how many batches of data are in our dataset\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- int -- corresponds to the number of batches in our dataset\n",
        "\t\t'''\n",
        "\t\treturn math.floor(self.images/batch_size)\n",
        "\n",
        "\t\n",
        "\tdef get_next_batch(self):\n",
        "\t\t'''\tResponsible for batching the data arrays and returning them\n",
        "\t\t\n",
        "\t\t\tReturns: \n",
        "\t\t\t\tlabel_batch: arr -- batch of labels for the associated image\n",
        "\t\t\t\timage_batch: arr -- batch of images for the associated labels\n",
        "\t\t'''\n",
        "\t\tstart_pos = (self.current_batch)\n",
        "\t\tend_pos =  (self.current_batch+1)\n",
        "\t\tprint(\"Called start\", start_pos)\n",
        "\t\tprint(\"end \", end_pos)\n",
        "\t\tlabel_batch = []\n",
        "\t\timage_batch = []\n",
        "\t\tif not self.data_dictionary:\n",
        "\t\t\tlabel_batch = self.labels[start_pos:end_pos]\n",
        "\t\t\timage_batch = self.images[start_pos:end_pos]\n",
        "\t\telse:\n",
        "\t\t\tlabel_batch_keys = self.data_keys[start_pos:end_pos]\n",
        "\t\t\tfor key in label_batch_keys:\n",
        "\t\t\t\tprint(\"keys \", key)        \n",
        "\t\t\t\timage_batch.append(np.resize(np.array(self.data_dictionary[key]['image_arr']), (256, 256, 1)))\n",
        "\t\t\t\tlabel_batch.append(self.data_dictionary[key]['labels'])\n",
        "\n",
        "\t\t# Reset the current batch once we've iterated through all of our data\n",
        "\t\tself.current_batch += 1\n",
        "\t\tif(self.current_batch >= self.batch_size):\n",
        "\t\t\tself.current_batch = 0\n",
        "\n",
        "\t\treturn label_batch, image_batch\n",
        "\t\t\n",
        "\tdef fetch_data(self, path_to_csv):\n",
        "\t\t''' Handles fetching the data from the DICOM Importer\n",
        "\t\t\n",
        "\t\t\tAssigns:\n",
        "\t\t\t\tself.labels\n",
        "\t\t\t\tself.images\n",
        "\t\t'''\n",
        "\t\timages_arr, labels_arr = self.import_labels_from_csv(path_to_csv)\n",
        "\t\tself.images = images_arr\n",
        "\t\tself.labels = labels_arr\n",
        "\t\t\n",
        "\tdef import_labels_from_csv(self, path):\n",
        "\t\t''' Handles opening a CSV of data and reading in the information to match\n",
        "\t\t\tThe image with the label.\n",
        "\t\t\t\n",
        "\t\t\tInput: \n",
        "\t\t\t\t- path: String -- path to the CSV\n",
        "\t\t\t\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- images: list -- list of image file names\n",
        "\t\t\t\t- labels: list -- list of boolean labels\n",
        "\t\t'''\n",
        "\t\tcsv_dataframe = pd.read_csv(path)\n",
        "\t\timages = list(csv_dataframe['file_name'])\n",
        "\t\tlabels = list(csv_dataframe['has_tbi'])\n",
        "\t\treturn images, labels\n",
        "\n",
        "\tdef open_csv(self, path):\n",
        "\t\t''' Handles opening a labels CSV for the test set and returning the datframe\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- path: String -- path to CSV\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- csv_dataframe: pandas dataframe for labels\n",
        "\n",
        "\t\t'''\n",
        "\t\tcsv_dataframe = pd.read_csv(path)\n",
        "\t\treturn csv_dataframe\n",
        "  \n",
        "\tdef scale_image(self, image, width, height):\n",
        "\t\t''' Handles scaling an image so that it can be fed into the UNET.\n",
        "\n",
        "\t\tInput:\n",
        "\t\t\t\t- image: A PIL Image instance\n",
        "\t\t\t\t- width: the new width we want the image to be\n",
        "\t\t\t\t- height: the new height we want the image to be\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\t- A PIL image instance that has been scaled\n",
        "\n",
        "\t\t\t\t* Will apply antialiasing to the \n",
        "\t\t'''\n",
        "\t\treturn image.resize((width, height), Image.ANTIALIAS)\n",
        "\n",
        "\n",
        "\tdef fetch_images_with_csv(self, path, dataframe):\n",
        "\t\t''' Handles fetching images from a filepath and constructs a dictionary with their labels\n",
        "\n",
        "\t\t\tInput:\n",
        "\t\t\t\t- path: String -- path to data folder\n",
        "\t\t\t\t- dataframe: pandas dataframe\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t- Dictionary of data structured as:\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\timage_name : {\n",
        "\t\t\t\t\t\timage_arr: [2D pixel array],\n",
        "\t\t\t\t\t\tlabels: [labels array]\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t'''\n",
        "\t\tdata_dictionary = {}\n",
        "\t\tcount = 0\n",
        "\t\tfor row in tqdm(dataframe.iterrows()):\n",
        "\t\t\tdata_dictionary[row[1][0]] = {}\n",
        "\t\t\timage_path = path +'/' + row[1][0] + '_blue.png'\n",
        "\t\t\timage = list(self.scale_image(Image.open(image_path), 256, 256).getdata())\n",
        "\t\t\tdata_dictionary[row[1][0]]['image_arr'] = image\n",
        "\t\t\tlabel_classes_arr = np.zeros(shape=(28))\n",
        "\t\t\tlabels_in_data = np.array(row[1][1].split(' '))\n",
        "\t\t\tprint(self.label_classes)\n",
        "\t\t\tprint(labels_in_data)\n",
        "\t\t\tfor label in labels_in_data:\n",
        "\t\t\t\tlabel_idx = int(label)\n",
        "\t\t\t\tprint(label_idx)\n",
        "\t\t\t\tprint(labels_in_data.size)\n",
        "\t\t\t\tlabel_classes_arr[label_idx] = 1\n",
        "\t\t\tprint(label_classes_arr)\n",
        "\t\t\tdata_dictionary[row[1][0]]['labels'] = label_classes_arr\n",
        "\t\t\tif count == self.batch_size:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tcount += 1\n",
        "\t\treturn data_dictionary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTMEK-5D_VmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Authors: Wezley Sherman\n",
        "#\n",
        "# Reference Attributes: U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "# Authors: Olaf Ronneberger, Philipp Fischer, and Thomas Brox\n",
        "#\n",
        "# This class is a part of the BSSCS Net Framework to import DICOM images and batch them for the UNET\n",
        "#\n",
        "# BSSCS Docs Importer location: TBD\n",
        "#\n",
        "# Citation:\n",
        "# Ronneberger, et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” \n",
        "#     [Astro-Ph/0005112] A Determination of the Hubble Constant from Cepheid Distances and a Model of the Local Peculiar Velocity Field, \n",
        "#     American Physical Society, 18 May 2015, arxiv.org/abs/1505.04597.\n",
        "# Site:\n",
        "# https://arxiv.org/pdf/1505.04597.pdf\n",
        "import tensorflow as tf\n",
        "\n",
        "class BSSCS_UNET:\n",
        "  def __init__(self, iterations, batch_size, data_class, labels_shape=[None, 1], learning_rate=0.0001):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.iterations = iterations\n",
        "    self.batch_size = batch_size\n",
        "    self.data_class = data_class\n",
        "    self.labels_shape = labels_shape\n",
        "\n",
        "\n",
        "  def generate_unet_arch(self, input):\n",
        "    ''' Handles generating a TF Implementation of a UNET utilizing the architecture discussed in\n",
        "    \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" by Ronneberger, Fischer, and Brox\n",
        "\n",
        "    The architecture for the UNET is not ours and all accrediation goes to Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \n",
        "    We are not claiming any ownership for the architecture. \n",
        "    Implementing the UNET arch is comparable to implementing selection sort.\n",
        "\n",
        "    tf.layers.conv2d docs: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d\n",
        "    tf.layers.conv2d_transpost for up convolutions: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose\n",
        "    tf.concat for the copy and crop methods: https://www.tensorflow.org/api_docs/python/tf/concat\n",
        "    tf.slice for cropping the tensors: https://www.tensorflow.org/api_docs/python/tf/slice\n",
        "    Hopefully I implemented this correctly  ¯\\_(ツ)_/¯\n",
        "\n",
        "    Quick guide on cropping a tensor -..\n",
        "\n",
        "    So after some research through the doc's I found out that we can't just crop it as if it were an image, because we are dealing with Tensors (matricies of data).\n",
        "\n",
        "    In order to crop a tensor we must use TensorFlow's slice function (https://www.tensorflow.org/api_docs/python/tf/slice)\n",
        "\n",
        "    Here we are cropping the convolutional layer we are upsampling to be the size of the convolutional layer we are concating to. \n",
        "    I'm starting at the base coordinates for the tensor object, and am cropping JUST the images (or filters). Thus why we have [-1, size_x, size_y, -1]. \n",
        "    The '-1' values are there to ensure we are keeping the remaining elements of the dimension (AKA our batch size and number of filters) . \n",
        "    From TF Docs on the -1 values: \" If size[i] is -1, all remaining elements in dimension i are included in the slice. In other words, this is equivalent to setting: size[i] = input.dim_size(i) - begin[i]\"\n",
        "\n",
        "    Once the tensor is properly cropped (Where each filter is the same size as the tensor we are copying into), we can concat the tensors. \n",
        "    This allows us to copy in all of the previous filters into the current tensor. The final shape will be: [Batch, Img_X, Img_Y, [Filters_A + Filters_B]]\n",
        "    '''\n",
        "    # first block in UNET --> Concat with the final block\n",
        "    convolution_layer_1 = tf.layers.conv2d(inputs=input, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_2 = tf.layers.conv2d(inputs=convolution_layer_1, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    max_pooling_layer_1 = tf.layers.max_pooling2d(inputs=convolution_layer_2, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "    # second block in UNET --> Concat with second to final block\n",
        "    convolution_layer_3 = tf.layers.conv2d(inputs=max_pooling_layer_1, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_4 = tf.layers.conv2d(inputs=convolution_layer_3, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    max_pooling_layer_2 = tf.layers.max_pooling2d(inputs=convolution_layer_4, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "    # third block in UNET --> Concat with third from final block\n",
        "    convolution_layer_5 = tf.layers.conv2d(inputs=max_pooling_layer_2, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_6 = tf.layers.conv2d(inputs=convolution_layer_5, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    max_pooling_layer_3 = tf.layers.max_pooling2d(inputs=convolution_layer_6, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "    # fourth block in UNET --> Concat with fourth from final block\n",
        "    convolution_layer_7 = tf.layers.conv2d(inputs=max_pooling_layer_3, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_8 = tf.layers.conv2d(inputs=convolution_layer_7, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    max_pooling_layer_4 = tf.layers.max_pooling2d(inputs=convolution_layer_8, pool_size=[2, 2], strides=1, padding=\"VALID\")\n",
        "\n",
        "    # middle UNET block\n",
        "    convolution_layer_9 = tf.layers.conv2d(inputs=max_pooling_layer_4, filters=1024, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_10 = tf.layers.conv2d(inputs=convolution_layer_9, filters=1024, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_up_1 = tf.layers.conv2d_transpose(inputs=convolution_layer_10, filters=1024, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "    # fourth from last \n",
        "    convolution_layer_8 = tf.slice(convolution_layer_8, [0, 0, 0, 0], [-1, convolution_up_1.shape[1], convolution_up_1.shape[2], -1])\n",
        "    concat_layer_1 = tf.concat([convolution_up_1, convolution_layer_8], axis=3) # Note: Experiment with the axis to ensure it is correct. Are we copying the batches or the filters? -- However; different axis's cause an error.\n",
        "    # print(concat_layer_1.shape) # Comes out to be [Batch_Size, Image_X, Image_Y, (Filters_Conv_8 + Filters_Conv_Up_1)]\n",
        "    convolution_layer_11 = tf.layers.conv2d(inputs=concat_layer_1, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_12 = tf.layers.conv2d(inputs=convolution_layer_11, filters=512, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_up_2 = tf.layers.conv2d_transpose(inputs=convolution_layer_12, filters=512, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "\n",
        "    # third from last\n",
        "    convolution_layer_6 = tf.slice(convolution_layer_6, [0, 0, 0, 0], [-1, convolution_up_2.shape[1], convolution_up_2.shape[2], -1])\n",
        "    concat_layer_1 = tf.concat([convolution_up_2, convolution_layer_6], axis=3)\n",
        "    convolution_layer_13 = tf.layers.conv2d(inputs=convolution_up_2, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_14 = tf.layers.conv2d(inputs=convolution_layer_13, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_up_3 = tf.layers.conv2d_transpose(inputs=convolution_layer_14, filters=256, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "    # second from last\n",
        "    convolution_layer_4 = tf.slice(convolution_layer_4, [0, 0, 0, 0], [-1, convolution_up_3.shape[1], convolution_up_3.shape[2], -1])\n",
        "    concat_layer_1 = tf.concat([convolution_up_3, convolution_layer_4], axis=3)\n",
        "    convolution_layer_15 = tf.layers.conv2d(inputs=convolution_up_3, filters=256, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_16 = tf.layers.conv2d(inputs=convolution_layer_15, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_up_4 = tf.layers.conv2d_transpose(inputs=convolution_layer_16, filters=128, kernel_size=[2, 2], strides=1, padding=\"SAME\")\n",
        "\n",
        "    # last block\n",
        "    convolution_layer_2 = tf.slice(convolution_layer_2, [0, 0, 0, 0], [-1, convolution_up_4.shape[1], convolution_up_4.shape[2], -1])\n",
        "    concat_layer_1 = tf.concat([convolution_up_4, convolution_layer_2], axis=3)\n",
        "    convolution_layer_17 = tf.layers.conv2d(inputs=convolution_up_4, filters=128, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_18 = tf.layers.conv2d(inputs=convolution_layer_17, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_layer_19 = tf.layers.conv2d(inputs=convolution_layer_18, filters=64, kernel_size=[3, 3], strides=1, padding=\"SAME\", activation=tf.nn.relu)\n",
        "    convolution_up_5 = tf.layers.conv2d_transpose(inputs=convolution_layer_19, filters=2, kernel_size=[1, 1], strides=1, padding=\"SAME\")\n",
        "    print(convolution_up_5.shape)\n",
        "    flattened = tf.reshape(convolution_up_5, [-1, 504])\n",
        "    return flattened\n",
        "\n",
        "  def create_regressor(self, input): \n",
        "    ''' Handles creating the regressor for the UNET classification\n",
        "\n",
        "        Parameters:\n",
        "          - input -- input layer (flattened layer from UNET) \n",
        "        Returns:\n",
        "          - Tensor -- last layer in the regressor\n",
        "    '''\n",
        "    reg_input = tf.layers.dense(inputs=input, units=504, activation=tf.nn.relu)\n",
        "    reg_hidden = tf.layers.dense(inputs=reg_input, units=800, activation=tf.nn.relu)\n",
        "    reg_hidden1 = tf.layers.dense(inputs=reg_hidden, units=1000, activation=tf.nn.relu)\n",
        "    reg_hidden2 = tf.layers.dense(inputs=reg_hidden1, units=1500, activation=tf.nn.relu)\n",
        "    reg_out = tf.layers.dense(inputs=reg_hidden2, units=28)\n",
        "    return reg_out\n",
        "\n",
        "  def create_loss(self, input, labels):\n",
        "    ''' Handles creating a loss function and returning it to the optimizer\n",
        "\n",
        "      Parameters:\n",
        "        - Input: The final layer in the graph we are computing the loss for\n",
        "        - Labels: The labels for the batch we are computing the loss for\n",
        "\n",
        "      Returns:\n",
        "        - Defined loss function\n",
        "\n",
        "      TensorFlow documentation: \n",
        "      https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
        "    '''\n",
        "    return tf.nn.softmax_cross_entropy_with_logits_v2(logits=input, labels=labels)\n",
        "\n",
        "  def create_optimizer(self, input, labels):\n",
        "    '''\n",
        "\n",
        "      TensorFlow documentation: \n",
        "      https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
        "    '''\n",
        "    loss = tf.square(tf.reduce_mean(self.create_loss(input, labels)))\n",
        "    return tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
        "\n",
        "  def train_unet(self):\n",
        "    ''' Handles training a UNET based off the data fed to it\n",
        "    '''\n",
        "\n",
        "\n",
        "    # This is where I would put my loss and optimization functions -..\n",
        "    # ..\n",
        "    # ..\n",
        "    # IF I HAD ONE!\n",
        "    #\n",
        "    # Meme Reference: https://www.youtube.com/watch?v=ciWPFvLS5IY\n",
        "    #\n",
        "    # On a serious note -.. Here is where we will plug in the deep regressor once that's built.\n",
        "    # After a UNET run the image will be passed to the deep regressor.\n",
        "    # The regressor will contain the loss function we are optimizing to.\n",
        "    input_ph = tf.placeholder(tf.float32, shape=[None, 256, 256, 1]) # Placeholder vals were given by paper in initial layer -- these numbers were referenced from the paper.\n",
        "    conv_input = self.generate_unet_arch(input_ph)\n",
        "    classifier = self.create_regressor(conv_input)\n",
        "    labels_placeholder = tf.placeholder(tf.float32, shape=self.labels_shape)\n",
        "    optimizer = self.create_optimizer(classifier, labels_placeholder)\n",
        "    loss = self.create_loss(classifier, labels_placeholder)\n",
        "    \n",
        "    unet_save = tf.train.Saver()\n",
        "      \n",
        "    with tf.Session() as session:\n",
        "      tf.global_variables_initializer().run()\n",
        "      for iteration in range(0, self.iterations): # counts for epochs -- or how many times we go through our data\n",
        "        for batch in range(0, self.batch_size):\n",
        "          y_b, X_b = self.data_class.get_next_batch()\n",
        "          #print(\"X shape: \", X_b)\n",
        "          #print(\"Y shape: \", y_b)\n",
        "          session.run(optimizer, feed_dict={input_ph:X_b, labels_placeholder:y_b})\n",
        "        if iteration % 10 == 0:\n",
        "          print(\"Iteration: \", iteration)\n",
        "          it_loss = session.run(loss, feed_dict={input_ph:X_b, labels_placeholder:y_b})\n",
        "          labels = session.run(classifier, feed_dict={input_ph: X_b})\n",
        "          print(self.save_graph(unet_save, session, \"output\"))\n",
        "          print(\"Predicted labels: \", labels)\n",
        "          print(\"Labels actual: \", y_b)\n",
        "          # Evaluate mse loss here and print the value\n",
        "          print(\"Passed 100 iterations with mse: \", it_loss)\n",
        "\n",
        "\n",
        "  def generalize_prediction(output):\n",
        "    ''' Handles generalizing UNET outputs to the labels so that we can better understand the predictions.\n",
        "        If the prediction is under 50% then we mark it as 0, otherwise keep the value.\n",
        "\n",
        "        Input: \n",
        "            - output - array of outputs based off labels from UNET\n",
        "\n",
        "        Return:\n",
        "            - return_prediction - array of normalized outputs from output input\n",
        "    '''\n",
        "    return_prediction = []\n",
        "    for prediction in output:\n",
        "      if(prediction > 0.5):\n",
        "        return_prediction.append(prediction)\n",
        "      else:\n",
        "        return_prediction.append(0)\n",
        "\n",
        "    return return_prediction\n",
        "\n",
        "  def test_unet(self, graph_out, input_x):\n",
        "    ''' Runs a trained UNET through an evaluation/test phase to detect errors\n",
        "\n",
        "        Parameters:\n",
        "          - graph_out: conv2d_tranpose tensor - The last layer in the graph\n",
        "          - input_x: image_arr  - Image array of shape [None, x, y, 1]\n",
        "\n",
        "        Returns:\n",
        "          - output: Returns the output of the unet graph\n",
        "    '''\n",
        "    with tf.Session() as session:\n",
        "      output = session.run(graph_out, feed_dict={input:input_x})\n",
        "\n",
        "    return output\n",
        "\n",
        "  def save_graph(self, unet_save, session, path):\n",
        "    ''' Saves a UNET graph\n",
        "        \n",
        "        Handles saving a UNET Graph to a specified path.\n",
        "        \n",
        "        Inputs:\n",
        "          - unet_save -- TF Saver instance\n",
        "          - session -- TF Session instance\n",
        "          - path -- path to save location\n",
        "        \n",
        "        Returns:\n",
        "          - String -- path to output unet\n",
        "          \n",
        "          \n",
        "        Save referenced from TensorFlow Documentation:\n",
        "        - https://www.tensorflow.org/guide/saved_model\n",
        "    '''\n",
        "    return unet_save.save(session, path)\n",
        "\n",
        "  def load_graph(self, unet_save, session, path):\n",
        "    ''' Loads a UNET graph\n",
        "    \n",
        "        Inputs:\n",
        "          - unet_save -- TF Saver instance\n",
        "          - session -- TF Session instance\n",
        "          - path -- path to save location\n",
        "        \n",
        "        Returns:\n",
        "          - None\n",
        "   \n",
        "        Load referenced from TensorFlow Documentation:\n",
        "        - https://www.tensorflow.org/guide/saved_model\n",
        "    '''\n",
        "    unet_save.restore(session, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcDoVC0wUgzQ",
        "colab_type": "code",
        "outputId": "adbd0bf8-50e4-4afe-d69b-27d278ca19d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_train   sample_data\t    test.zip   train.zip\n",
            "kaggle.json  sample_submission.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YWvL-RmfBBst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def test_unet_training(image_path, csv_path, batch_size, iterations):\n",
        "  unet_data = UNET_DATA(label_classes=None, images_path=image_path, csv_path=csv_path)\n",
        "  unet = BSSCS_UNET(iterations, batch_size, unet_data, labels_shape=[None, 28])\n",
        "  print(unet_data.get_next_batch())\n",
        "  unet.train_unet()\n",
        "\n",
        "test_unet_training(image_path=\"data_train\", csv_path=\"train.csv\", batch_size=10, iterations=31)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOvRtGHUezN_",
        "colab_type": "code",
        "outputId": "4c98151f-9a9f-474f-8e50-d3bb4c55a43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir='/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorBoard 1.12.2 at http://a69bb1cc68f1:6006 (Press CTRL+C to quit)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}